# TagWriting

TagWriting is a CLI tool that enables fast and flexible text generation by simply enclosing prompts in tags within your text files. It is designed to be simple, stateless, and editor-agnostic, making it easy to integrate into any workflow.

---

## Overview

TagWriting is a tool that connects AI and humans more seamlessly through text files. By monitoring a directory, TagWriting will automatically convert text or Markdown files as soon as they are saved.

```markdown
I am TagWriting.
<prompt>Describe the best feature of TagWriting in one sentence.</prompt>
```

↓

```markdown
I am TagWriting.
You can quickly generate text just by enclosing it in tags.
```

---

## Usage

1. Edit a file such as `.md` and enclose your prompt in tags  
2. When you save, the tagged section is converted by the LLM  
3. The result is written directly to the file

---

## Use Case: Prompt Chaining

With TagWriting, you can perform stepwise generation simply by placing multiple prompts in sequence.

```markdown
I am TagWriting.
<prompt>Why did you create this product?</prompt>
<prompt>Describe the best feature of TagWriting in one sentence.</prompt>
```

After saving:

```markdown
I am TagWriting.
I came up with the idea when I was looking for a better way to create documents using text generation AI.
<prompt>Describe the best feature of TagWriting in one sentence.</prompt>
```

---

## Why TagWriting?

### Seamless with Text Editing
Just enclose prompts directly in your text with tags. No need to stop your workflow to operate the LLM. You can leverage AI without interrupting your train of thought.

### High Readability
By explicitly writing tags, it’s clear which parts you want the AI to handle. Document history and edits are also clear.

### Flexibility & Compatibility
TagWriting directly rewrites updated text files. In theory, it works with any editor and any format. As long as your editor supports file reload, you’re ready to go—no plugins needed. Use Visual Studio Code, Vim, Emacs, etc.—whatever you like.

---

## Installation (Python)

1. Install dependencies:

```sh
pip install .
```

2. Use as a command-line tool:

```sh
tagwriting <directory>
```

---

## How to use .env

Create a `.env` file in your project directory and specify your API key, model name, and base URL as follows:

```env
API_KEY=sk-xxxxxxx
MODEL=gpt-3.5-turbo
BASE_URL=https://api.openai.com/v1
```

- The `.env` file in the directory where you run the `tagwriting` command will be loaded automatically.
- If you want to use different settings for multiple projects, prepare a separate `.env` for each directory.
- Any OpenAPI-compatible endpoint can be used (e.g., Grok, Deepseek, etc.).

---

## System Structure

### `tagwriting <directory>`

Monitors the specified directory, searching for and processing `<prompt>` tags in saved files.

- Requests to LLMs or external APIs are made asynchronously
- Errors and status are output to the CLI in real time
- Updates are written directly to the file (re-saving triggers re-processing)

---

## About Development

TagWriting is currently in experimental development.  
Design principles:

- Simple: Detect tags → send context → replace with results
- Stateless: File-based, minimal environment dependency
- Minimal interface: Usable with any tool, no dependency on UI

---

## Behavior Specifications

Internally, TagWriting works as follows:

1. Monitor directory
2. Detect file changes in the directory
3. Read the file
4. Fire event
5. Detect template tags (e.g., `<summary></summary>`)
6. Convert template tags to prompt tags (e.g., `<prompt>Summarize the entire text</prompt>`)
7. Detect prompt tags (e.g., `<prompt>Summarize the entire text</prompt>`)
8. Mark the conversion part of the prompt (e.g., `@@prompt@@`)
9. Write the result to the file (e.g., `TagWriting is a great product`)
10. Resume monitoring the directory

## Built-in Tag Details

### prompt tag (`<prompt></prompt>`)

This is the tag processed last, and the text inside the prompt tag is sent as an instruction to the LLM. The context used is the entire text (including any expanded include tags).

For example:

```markdown
I am TagWriting.
<prompt>Why did you create this product?</prompt>
```

will be sent to the LLM as a prompt like:

```
  Your answer will replace `@@processing@@` in the context. Output text according to the context's consistency.
  Rule:
   - Do not include `@@processing@@` in your answer.
   - Do not include explanations; answer the user prompt directly.
  context:
  I am TagWriting.
  @@processing@@
  user prompt:
  Why did you create this product?
```

#### Specifications

The text generated by the prompt tag is replaced within the `<prompt></prompt>` tag to ensure stoppability. Even if you instruct recursively to enclose with `<prompt></prompt>` tags, those tags will be removed.

```markdown
I am TagWriting.
<prompt>Then, using the `prompt tag`, enclose "apple".</prompt>
```

```markdown
I am TagWriting.
apple
```

Even if `<prompt>apple</prompt>` was output, only "apple" will appear.

### include tag (`<include>filepath</include>`)

A special tag to insert the contents of the specified file into the LLM prompt.

- Format: `<include>path</include>` (e.g., `<include>foo/bar.txt</include>`)
- The path is resolved relative to the current file being processed.
- The include tag is replaced entirely with the contents of the specified file.
- If there are multiple include tags, all are expanded at once.
- **Current spec**: Nested includes (an include inside an included file) are expanded only one level.
- **Current spec**: `<prompt></prompt>` tags inside included files are not expanded.

#### Example

```markdown
# Main text
<include>foo.md</include>
<include>bar.md</include>
```

When saved, the contents of `foo.md` and `bar.md` are inserted at their respective locations.

## YAML Template System

TagWriting supports a template system using YAML files. This allows you to flexibly define custom tags, prompt formats, files to ignore, and more.

### Example of sample.yaml

```yaml
prompt: |
  Your answer will replace `@@processing@@` in the context. Output text according to the context's consistency.
  Rule:
   - Do not include `@@processing@@` in your answer.
   - Do not include explanations; answer the user prompt directly.
  context:
  {prompt_text}
  user prompt:
  {prompt}

ignore:
  - "*.py"
  - "*.yaml"
  - "README.md"
  - "sandbox/test_not_target.md"
  - ".git"

tags:
  - tag: "detail"
    format: "Describe in detail: {prompt}"
  - tag: "summary"
    format: "Summarize the entire text"
  - tag: "profile"
    format: "Generate a profile for a person. Enclose each item except the name with <detail></detail>: {prompt}"
  - tag: "prm"
    format: "{prompt}"
```

### Command Example

```sh
tagwriting ./foobar/path --templates sample.yaml
```

### Template Format

- `prompt`: The overall template sent to the LLM. `{prompt}` and `{prompt_text}` are available.
- `ignore`: List of files or patterns to ignore.
- `tags`: List of custom tags. Each tag has `tag` and `format`, and `{prompt}` will be replaced with the tag's text.

#### Custom Tag Example

In a Markdown file:

```markdown
<detail>Please explain this story.</detail>
```

will be automatically converted according to the template to:

```markdown
<prompt>Describe in detail: Please explain this story.</prompt>
```

Then, it will be processed by the LLM.
