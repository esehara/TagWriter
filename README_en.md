# TagWriting

TagWriting is a CLI tool that enables fast and flexible text generation by simply enclosing prompts in tags within your text files. It is designed to be simple, stateless, and editor-agnostic, making it easy to integrate into any workflow.

---

## Overview

TagWriting is a tool that connects AI and humans more seamlessly through text files. By monitoring a directory, TagWriting will automatically convert text or Markdown files as soon as they are saved.

```markdown
I am TagWriting.
<prompt>Describe the best feature of TagWriting in one sentence.</prompt>
```

‚Üì

```markdown
I am TagWriting.
You can quickly generate text just by enclosing it in tags.
```

---

## Usage

1. Edit a file such as `.md` and enclose your prompt in tags  
2. When you save, the tagged section is converted by the LLM  
3. The result is written directly to the file

---

## Use Case: Prompt Chaining

With TagWriting, you can perform stepwise generation simply by placing multiple prompts in sequence.

```markdown
I am TagWriting.
<prompt>Why did you create this product?</prompt>
<prompt>Describe the best feature of TagWriting in one sentence.</prompt>
```

After saving:

```markdown
I am TagWriting.
I came up with the idea when I was looking for a better way to create documents using text generation AI.
<prompt>Describe the best feature of TagWriting in one sentence.</prompt>
```

---

## Why TagWriting?

### Seamless with Text Editing
Just enclose prompts directly in your text with tags. No need to stop your workflow to operate the LLM. You can leverage AI without interrupting your train of thought.

### High Readability
By explicitly writing tags, it‚Äôs clear which parts you want the AI to handle. Document history and edits are also clear.

### Flexibility & Compatibility
TagWriting directly rewrites updated text files. In theory, it works with any editor and any format. As long as your editor supports file reload, you‚Äôre ready to go‚Äîno plugins needed. Use Visual Studio Code, Vim, Emacs, etc.‚Äîwhatever you like.

---

## Installation (Python)

1. Install dependencies:

```sh
pip install .
```

2. Use as a command-line tool:

```sh
tagwriting <directory>
```

---

## How to use .env

Create a `.env` file in your project directory and specify your API key, model name, and base URL as follows:

```env
API_KEY=sk-xxxxxxx
MODEL=gpt-3.5-turbo
BASE_URL=https://api.openai.com/v1
```

- The `.env` file in the directory where you run the `tagwriting` command will be loaded automatically.
- If you want to use different settings for multiple projects, prepare a separate `.env` for each directory.
- Any OpenAPI-compatible endpoint can be used (e.g., Grok, Deepseek, etc.).

---

## System Structure

### `tagwriting <directory>`

Monitors the specified directory, searching for and processing `<prompt>` tags in saved files.

- Requests to LLMs or external APIs are made asynchronously
- Errors and status are output to the CLI in real time
- Updates are written directly to the file (re-saving triggers re-processing)

---

## About Development

TagWriting is currently in experimental development.  
Design principles:

- Simple: Detect tags ‚Üí send context ‚Üí replace with results
- Stateless: File-based, minimal environment dependency
- Minimal interface: Usable with any tool, no dependency on UI

---

## Behavior Specifications

Internally, TagWriting works as follows:

1. Monitor directory
2. Detect file changes in the directory
3. Read the file
4. Fire event
5. Detect template tags (e.g., `<prompt>ÊñáÁ´†ÂÖ®‰Ωì„ÇíË¶ÅÁ¥Ñ„Åô„Çã</prompt>`)
6. Convert template tags to prompt tags (e.g., `<prompt>Summarize the entire text</prompt>`)
7. Detect prompt tags (e.g., `<prompt>Summarize the entire text</prompt>`)
8. Mark the conversion part of the prompt (e.g., `@@prompt@@`)
9. Write the result to the file (e.g., `TagWriting is a great product`)
10. Resume monitoring the directory

## Built-in Tag Details

### prompt tag (`<prompt></prompt>`)

This is the tag processed last, and the text inside the prompt tag is sent as an instruction to the LLM. The context used is the entire text (including any expanded include tags).

For example:

```markdown
I am TagWriting.
<prompt>Why did you create this product?</prompt>
```

will be sent to the LLM as a prompt like:

```
  Your answer will replace `@@processing@@` in the context. Output text according to the context's consistency.
  Rule:
   - Do not include `@@processing@@` in your answer.
   - Do not include explanations; answer the user prompt directly.
  context:
  I am TagWriting.
  @@processing@@
  user prompt:
  Why did you create this product?
```

#### Specifications

The text generated by the prompt tag is replaced within the `<prompt></prompt>` tag to ensure stoppability. Even if you instruct recursively to enclose with `<prompt></prompt>` tags, those tags will be removed.

```markdown
I am TagWriting.
<prompt>Then, using the `prompt tag`, enclose "apple".</prompt>
```

```markdown
I am TagWriting.
apple
```

Even if `<prompt>apple</prompt>` was output, only "apple" will appear.

### include tag (`<include>filepath</include>`)

A special tag to insert the contents of the specified file into the LLM prompt.

- Format: `<include>path</include>` (e.g., `<include>foo/bar.txt</include>`)
- The path is resolved relative to the current file being processed.
- The include tag is replaced entirely with the contents of the specified file.
- If there are multiple include tags, all are expanded at once.
- **Current spec**: Nested includes (an include inside an included file) are expanded only one level.
- **Current spec**: `<prompt></prompt>` tags inside included files are not expanded.

#### Example

```markdown
# Main text
<include>foo.md</include>
<include>bar.md</include>
```

When saved, the contents of `foo.md` and `bar.md` are inserted at their respective locations.

## YAML Template System

TagWriting supports a template system using YAML files. This allows you to flexibly define custom tags, prompt formats, files to ignore, and more.

### Example of sample.yaml

```yaml
prompt: |
  Your answer will replace `@@processing@@` in the context. Output text according to the context's consistency.
  Rule:
   - Do not include `@@processing@@` in your answer.
   - Do not include explanations; answer the user prompt directly.
  context:
  {context}
  user prompt:
  {prompt}

# history:
#  file: "{filename}.history.md"
#  template: |
#    ---
#    Prompt: {prompt}
#    Result: {result}
#    Timestamp: {timestamp}

target:
  - "*.md"
  - "*.markdown"

ignore:
  - "*.py"
  - "*.yaml"
  - "README.md"
  - "sandbox/test_not_target.md"
  - ".git"

tags:
  - tag: "detail"
    format: "Describe in detail: {prompt}"
  - tag: "summary"
    format: "Summarize the entire text"
  - tag: "profile"
    format: "Generate a profile for a person. Enclose each item except the name with <prompt>Ë©≥Á¥∞„Å´Ë™¨Êòé„Åô„Çã: </prompt>: {prompt}"
  - tag: "prm"
    format: "{prompt}"
```

### Command Example

```sh
tagwriting ./foobar/path --templates sample.yaml
```

### Template Format

- `prompt`: The overall template sent to the LLM. `{prompt}` and `{prompt_text}` are available.
- `ignore`: List of files or patterns to ignore.
- `tags`: List of custom tags. Each tag has `tag` and `format`, and `{prompt}` will be replaced with the tag's text.

#### Custom Tag Example

In a Markdown file:

```markdown
<prompt>Ë©≥Á¥∞„Å´Ë™¨Êòé„Åô„Çã: Please explain this story.</prompt>
```

will be automatically converted according to the template to:

```markdown
<prompt>Describe in detail: Please explain this story.</prompt>
```

Then, it will be processed by the LLM.

---

## üè∑Ô∏è How to Use Attribute Prompt Rules

In TagWriting, you can assign explanations or instructions as "attribute prompt rules" to tag attributes (e.g., the `funny` part in `<prompt:funny>...</prompt>`).

### 1. Define attribute prompt rules in the YAML template

In templates like `sample.yaml`, set rules (explanations/instructions) for each attribute in the `attrs` section.

```yaml
attrs:
  funny: "Output in a humorous tone"
  detail: "Describe in detail"
```

### 2. Specify attributes in tags

Use tags with attributes in Markdown or text files.

```markdown
<prompt:funny>What kind of day is it?</prompt>
```

### 3. Attribute prompt rules are reflected in the prompt

The explanation for the specified attribute (attribute prompt rule) is automatically added to the `Rule` section of the prompt sent to the LLM.

**Example:**

```yaml
Rule:
 - Output in a humorous tone
 - Do not include `@@processing@@` in your answer.
 - Answer the user prompt directly, without explanations or commentary.
```

If multiple attributes are specified, all attribute prompt rules are added.

---

# FAQ

## What is the difference between attribute prompt rules and custom tags?

Attribute prompt rules are appended as rules in the prompt, so they can be reused flexibly. On the other hand, custom tags are conversions to other tags, so they are less flexible but more specific. For example, the following combination illustrates the difference well:

```yaml
attrs:
 - simple: "Explain concisely"

tags:
  - tag: "summary"
    format: "Summarize the entire text"
```

```
<summary:simple></summary>
```

In this case, "Explain concisely" is a reusable rule, while "summary" is a more specific instruction. Let's check how the prompt actually sent to the LLM looks:

```
  Your answer will replace `@@processing@@` in the context. Please output the text according to the context consistency.
  Rule:
   - Do not include `@@processing@@` in your answer.
   - Answer the user prompt directly, without explanations or commentary.
   - Explain concisely
  context:
  @@processing@@
  user prompt: 
  Summarize the entire text
```

## Do you plan to add post-processing after LLM responses?

Not really. Adding arbitrary triggers for "some preprocessing -> LLM -> some post-processing" is surprisingly complex, and mixing preprocessing and post-processing can make it hard to keep the feature simple and may confuse users. So, for now, it's low priority.

Also, according to TagWriting's design philosophy, its role is to "generate prompts on text files". In other words, post-processing of text generated by the LLM is not the role of this software but is left to the user.

For example, if you want to keep the user prompt used with the `<chat></chat>` tag after the LLM, consider using `attrs` for that purpose.

```
attrs:
   remain: "Keep the user prompt with a `> ` prefix"
```